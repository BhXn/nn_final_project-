{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 神经网络编程框架\n",
    "- 参考配置：\n",
    "    - 系统：Ubuntu 20.04\n",
    "    - CUDA：12.4\n",
    "    - gcc：9.5\n",
    "    - cmake：4.0.1\n",
    "\n",
    "- 作业要求\n",
    "    1. 实现ppt中列举的TensorOp子类并测试；\n",
    "    2. 实现ppt中列举的Module、Optimizer、Scheduler子类；\n",
    "    3. 使用简易框架进行ResidualMLP实验，实现模型存储和读取，测试阶段不计算梯度。\n",
    "\n",
    "完成上述要求后可以通过以下代码进行测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install --upgrade --no-deps git+https://github.com/dlsys10714/mugrade.git\n",
    "!pip3 install pybind11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%set_env PYTHONPATH ./python\n",
    "%set_env NEEDLE_BACKEND nd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./python')\n",
    "import needle as ndl\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "如下是一些简单的测试样例，不报错即为通过作业要求中的1。其余部分请自行设计测试用例，确保代码的正确性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EWiseAdd\n",
    "def test_add():\n",
    "    np.testing.assert_allclose(ndl.add(np.array([1,2]), np.array([3,4])).numpy(),\n",
    "                               np.array([4,6]))\n",
    "test_add()\n",
    "#scalar_add\n",
    "def test_scalar_add():\n",
    "    np.testing.assert_allclose(ndl.add_scalar(np.array([1,2]), 3).numpy(),\n",
    "                               np.array([4,5]))\n",
    "test_scalar_add()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_mul():\n",
    "    np.testing.assert_allclose(ndl.multiply(np.array([1,2]), np.array([3,4])).numpy(),\n",
    "                               np.array([3,8]))\n",
    "def test_scalar_mul():\n",
    "    np.testing.assert_allclose(ndl.mul_scalar(np.array([1,2]), 3).numpy(),\n",
    "                               np.array([3,6]))\n",
    "test_mul()\n",
    "test_scalar_mul()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_power_scalar_forward():\n",
    "    print(ndl.power_scalar(ndl.Tensor(2),scalar=2).numpy())\n",
    "    print(ndl.power_scalar(ndl.Tensor([[0.5, 2.0, 3.0]]), scalar=2).numpy())\n",
    "    np.testing.assert_allclose(\n",
    "        ndl.power_scalar(ndl.Tensor([[0.5, 2.0, 3.0]]), scalar=2).numpy(),\n",
    "        np.array([[0.25, 4.0, 9.0]]),\n",
    "    )\n",
    "test_power_scalar_forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_divide_forward():\n",
    "    np.testing.assert_allclose(\n",
    "        ndl.divide(\n",
    "            ndl.Tensor([[3.3, 4.35, 1.2], [2.45, 0.95, 2.55]]),\n",
    "            ndl.Tensor([[4.6, 4.35, 4.8], [0.65, 0.7, 4.4]]),\n",
    "        ).numpy(),\n",
    "        np.array(\n",
    "            [\n",
    "                [0.717391304348, 1.0, 0.25],\n",
    "                [3.769230769231, 1.357142857143, 0.579545454545],\n",
    "            ]\n",
    "        ),\n",
    "    )\n",
    "test_divide_forward()\n",
    "#scalerdiv\n",
    "def test_divide_scalar_forward():\n",
    "    np.testing.assert_allclose(\n",
    "        ndl.divide_scalar(ndl.Tensor([[1.7, 1.45]]), scalar=12).numpy(),\n",
    "        np.array([[0.141666666667, 0.120833333333]]),\n",
    "    )\n",
    "test_divide_scalar_forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transpose\n",
    "def test_transpose_forward():\n",
    "    np.testing.assert_allclose(\n",
    "        ndl.transpose(ndl.Tensor([[[1.95]], [[2.7]], [[3.75]]]), axes=(1, 2)).numpy(),\n",
    "        np.array([[[1.95]], [[2.7]], [[3.75]]]),\n",
    "    )\n",
    "    np.testing.assert_allclose(\n",
    "        ndl.transpose(\n",
    "            ndl.Tensor([[[[0.95]]], [[[2.55]]], [[[0.45]]]]), axes=(2, 3)\n",
    "        ).numpy(),\n",
    "        np.array([[[[0.95]]], [[[2.55]]], [[[0.45]]]]),\n",
    "    )\n",
    "    np.testing.assert_allclose(\n",
    "        ndl.transpose(\n",
    "            ndl.Tensor(\n",
    "                [\n",
    "                    [[[0.4, 0.05], [2.95, 1.3]], [[4.8, 1.2], [1.65, 3.1]]],\n",
    "                    [[[1.45, 3.05], [2.25, 0.1]], [[0.45, 4.75], [1.5, 1.8]]],\n",
    "                    [[[1.5, 4.65], [1.35, 2.7]], [[2.0, 1.65], [2.05, 1.2]]],\n",
    "                ]\n",
    "            )\n",
    "        ).numpy(),\n",
    "        np.array(\n",
    "            [\n",
    "                [[[0.4, 2.95], [0.05, 1.3]], [[4.8, 1.65], [1.2, 3.1]]],\n",
    "                [[[1.45, 2.25], [3.05, 0.1]], [[0.45, 1.5], [4.75, 1.8]]],\n",
    "                [[[1.5, 1.35], [4.65, 2.7]], [[2.0, 2.05], [1.65, 1.2]]],\n",
    "            ]\n",
    "        ),\n",
    "    )\n",
    "    np.testing.assert_allclose(\n",
    "        ndl.transpose(ndl.Tensor([[[2.45]], [[3.5]], [[0.9]]]), axes=(0, 1)).numpy(),\n",
    "        np.array([[[2.45], [3.5], [0.9]]]),\n",
    "    )\n",
    "    np.testing.assert_allclose(\n",
    "        ndl.transpose(ndl.Tensor([[4.4, 2.05], [1.85, 2.25], [0.15, 1.4]])).numpy(),\n",
    "        np.array([[4.4, 1.85, 0.15], [2.05, 2.25, 1.4]]),\n",
    "    )\n",
    "    np.testing.assert_allclose(\n",
    "        ndl.transpose(\n",
    "            ndl.Tensor([[0.05, 3.7, 1.35], [4.45, 3.25, 1.95], [2.45, 4.4, 4.5]])\n",
    "        ).numpy(),\n",
    "        np.array([[0.05, 4.45, 2.45], [3.7, 3.25, 4.4], [1.35, 1.95, 4.5]]),\n",
    "    )\n",
    "    np.testing.assert_allclose(\n",
    "        ndl.transpose(\n",
    "            ndl.Tensor(\n",
    "                [\n",
    "                    [[0.55, 1.8, 0.2], [0.8, 2.75, 3.7], [0.95, 1.4, 0.8]],\n",
    "                    [[0.75, 1.6, 1.35], [3.75, 4.0, 4.55], [1.85, 2.5, 4.8]],\n",
    "                    [[0.2, 3.35, 3.4], [0.3, 4.85, 4.85], [4.35, 4.25, 3.05]],\n",
    "                ]\n",
    "            ),\n",
    "            axes=(0, 1),\n",
    "        ).numpy(),\n",
    "        np.array(\n",
    "            [\n",
    "                [[0.55, 1.8, 0.2], [0.75, 1.6, 1.35], [0.2, 3.35, 3.4]],\n",
    "                [[0.8, 2.75, 3.7], [3.75, 4.0, 4.55], [0.3, 4.85, 4.85]],\n",
    "                [[0.95, 1.4, 0.8], [1.85, 2.5, 4.8], [4.35, 4.25, 3.05]],\n",
    "            ]\n",
    "        ),\n",
    "    )\n",
    "test_transpose_forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_reshape_forward():\n",
    "    np.testing.assert_allclose(\n",
    "        ndl.reshape(\n",
    "            ndl.Tensor(\n",
    "                [\n",
    "                    [2.9, 2.0, 2.4],\n",
    "                    [3.95, 3.95, 4.65],\n",
    "                    [2.1, 2.5, 2.7],\n",
    "                    [1.9, 4.85, 3.25],\n",
    "                    [3.35, 3.45, 3.45],\n",
    "                ]\n",
    "            ),\n",
    "            shape=(15,),\n",
    "        ).numpy(),\n",
    "        np.array(\n",
    "            [\n",
    "                2.9,\n",
    "                2.0,\n",
    "                2.4,\n",
    "                3.95,\n",
    "                3.95,\n",
    "                4.65,\n",
    "                2.1,\n",
    "                2.5,\n",
    "                2.7,\n",
    "                1.9,\n",
    "                4.85,\n",
    "                3.25,\n",
    "                3.35,\n",
    "                3.45,\n",
    "                3.45,\n",
    "            ]\n",
    "        ),\n",
    "    )\n",
    "    np.testing.assert_allclose(\n",
    "        ndl.reshape(\n",
    "            ndl.Tensor(\n",
    "                [\n",
    "                    [[4.1, 4.05, 1.35, 1.65], [3.65, 0.9, 0.65, 4.15]],\n",
    "                    [[4.7, 1.4, 2.55, 4.8], [2.8, 1.75, 2.8, 0.6]],\n",
    "                    [[3.75, 0.6, 0.0, 3.5], [0.15, 1.9, 4.75, 2.8]],\n",
    "                ]\n",
    "            ),\n",
    "            shape=(2, 3, 4),\n",
    "        ).numpy(),\n",
    "        np.array(\n",
    "            [\n",
    "                [\n",
    "                    [4.1, 4.05, 1.35, 1.65],\n",
    "                    [3.65, 0.9, 0.65, 4.15],\n",
    "                    [4.7, 1.4, 2.55, 4.8],\n",
    "                ],\n",
    "                [[2.8, 1.75, 2.8, 0.6], [3.75, 0.6, 0.0, 3.5], [0.15, 1.9, 4.75, 2.8]],\n",
    "            ]\n",
    "        ),\n",
    "    )\n",
    "test_reshape_forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_broadcast_to_forward():\n",
    "    np.testing.assert_allclose(\n",
    "        ndl.broadcast_to(ndl.Tensor([[1.85, 0.85, 0.6]]), shape=(3, 3, 3)).numpy(),\n",
    "        np.array(\n",
    "            [\n",
    "                [[1.85, 0.85, 0.6], [1.85, 0.85, 0.6], [1.85, 0.85, 0.6]],\n",
    "                [[1.85, 0.85, 0.6], [1.85, 0.85, 0.6], [1.85, 0.85, 0.6]],\n",
    "                [[1.85, 0.85, 0.6], [1.85, 0.85, 0.6], [1.85, 0.85, 0.6]],\n",
    "            ]\n",
    "        ),\n",
    "    )\n",
    "test_broadcast_to_forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_summation_forward():\n",
    "    np.testing.assert_allclose(\n",
    "        ndl.summation(\n",
    "            ndl.Tensor(\n",
    "                [\n",
    "                    [2.2, 4.35, 1.4, 0.3, 2.65],\n",
    "                    [1.0, 0.85, 2.75, 3.8, 1.55],\n",
    "                    [3.2, 2.3, 3.45, 0.7, 0.0],\n",
    "                ]\n",
    "            )\n",
    "        ).numpy(),\n",
    "        np.array(30.5),\n",
    "    )\n",
    "    np.testing.assert_allclose(\n",
    "        ndl.summation(\n",
    "            ndl.Tensor(\n",
    "                [\n",
    "                    [1.05, 2.55, 1.0],\n",
    "                    [2.95, 3.7, 2.6],\n",
    "                    [0.1, 4.1, 3.3],\n",
    "                    [1.1, 3.4, 3.4],\n",
    "                    [1.8, 4.55, 2.3],\n",
    "                ]\n",
    "            ),\n",
    "            axes=1,\n",
    "        ).numpy(),\n",
    "        np.array([4.6, 9.25, 7.5, 7.9, 8.65]),\n",
    "    )\n",
    "    np.testing.assert_allclose(\n",
    "        ndl.summation(\n",
    "            ndl.Tensor([[1.5, 3.85, 3.45], [1.35, 1.3, 0.65], [2.6, 4.55, 0.25]]),\n",
    "            axes=0,\n",
    "        ).numpy(),\n",
    "        np.array([5.45, 9.7, 4.35]),\n",
    "    )\n",
    "test_summation_forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_matmul_forward():\n",
    "    np.testing.assert_allclose(\n",
    "        ndl.matmul(\n",
    "            ndl.Tensor([[4.95, 1.75, 0.25], [4.15, 4.25, 0.3], [0.3, 0.4, 2.1]]),\n",
    "            ndl.Tensor([[1.35, 2.2, 1.55], [3.85, 4.8, 2.6], [1.15, 0.85, 4.15]]),\n",
    "        ).numpy(),\n",
    "        np.array(\n",
    "            [[13.7075, 19.5025, 13.26], [22.31, 29.785, 18.7275], [4.36, 4.365, 10.22]]\n",
    "        ),\n",
    "    )\n",
    "    np.testing.assert_allclose(\n",
    "        ndl.matmul(\n",
    "            ndl.Tensor([[3.8, 0.05], [2.3, 3.35], [1.6, 2.6]]),\n",
    "            ndl.Tensor([[1.1, 3.5, 3.7], [0.05, 1.25, 1.0]]),\n",
    "        ).numpy(),\n",
    "        np.array(\n",
    "            [[4.1825, 13.3625, 14.11], [2.6975, 12.2375, 11.86], [1.89, 8.85, 8.52]]\n",
    "        ),\n",
    "    )\n",
    "test_matmul_forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_negate_forward():\n",
    "    np.testing.assert_allclose(\n",
    "        ndl.negate(ndl.Tensor([[1.45, 0.55]])).numpy(), np.array([[-1.45, -0.55]])\n",
    "    )\n",
    "test_negate_forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#求导\n",
    "def test_compute_gradient():\n",
    "    gradient_check(\n",
    "        lambda A, B, C: ndl.summation((A @ B + C) * (A @ B), axes=None),\n",
    "        ndl.Tensor(np.random.randn(10, 9)),\n",
    "        ndl.Tensor(np.random.randn(9, 8)),\n",
    "        ndl.Tensor(np.random.randn(10, 8)),\n",
    "        backward=True,\n",
    "    )\n",
    "    gradient_check(\n",
    "        lambda A, B: ndl.summation(ndl.broadcast_to(A, shape=(10, 9)) * B, axes=None),\n",
    "        ndl.Tensor(np.random.randn(10, 1)),\n",
    "        ndl.Tensor(np.random.randn(10, 9)),\n",
    "        backward=True,\n",
    "    )\n",
    "    gradient_check(\n",
    "        lambda A, B, C: ndl.summation(\n",
    "            ndl.reshape(A, shape=(10, 10)) @ B / 5 + C, axes=None\n",
    "        ),\n",
    "        ndl.Tensor(np.random.randn(100)),\n",
    "        ndl.Tensor(np.random.randn(10, 5)),\n",
    "        ndl.Tensor(np.random.randn(10, 5)),\n",
    "        backward=True,\n",
    "    )\n",
    "\n",
    "    # check gradient of gradient\n",
    "    x2 = ndl.Tensor([6])\n",
    "    x3 = ndl.Tensor([0])\n",
    "    y = x2 * x2 + x2 * x3\n",
    "    y.backward()\n",
    "    grad_x2 = x2.grad\n",
    "    grad_x3 = x3.grad\n",
    "    # gradient of gradient\n",
    "    grad_x2.backward()\n",
    "    grad_x2_x2 = x2.grad\n",
    "    grad_x2_x3 = x3.grad\n",
    "    x2_val = x2.numpy()\n",
    "    x3_val = x3.numpy()\n",
    "    assert y.numpy() == x2_val * x2_val + x2_val * x3_val\n",
    "    assert grad_x2.numpy() == 2 * x2_val + x3_val\n",
    "    assert grad_x3.numpy() == x2_val\n",
    "    assert grad_x2_x2.numpy() == 2\n",
    "    assert grad_x2_x3.numpy() == 1\n",
    "\n",
    "def gradient_check(f, *args, tol=1e-6, backward=False, **kwargs):\n",
    "       eps = 1e-4\n",
    "       numerical_grads = [np.zeros(a.shape) for a in args]\n",
    "       for i in range(len(args)):\n",
    "           for j in range(args[i].realize_cached_data().size):\n",
    "               args[i].realize_cached_data().flat[j] += eps\n",
    "               f1 = float(f(*args, **kwargs).numpy().sum())\n",
    "               args[i].realize_cached_data().flat[j] -= 2 * eps\n",
    "               f2 = float(f(*args, **kwargs).numpy().sum())\n",
    "               args[i].realize_cached_data().flat[j] += eps\n",
    "               numerical_grads[i].flat[j] = (f1 - f2) / (2 * eps)\n",
    "       if not backward:\n",
    "           out = f(*args, **kwargs)\n",
    "           computed_grads = [\n",
    "               x.numpy()\n",
    "               for x in out.op.gradient_as_tuple(ndl.Tensor(np.ones(out.shape)), out)\n",
    "           ]\n",
    "       else:\n",
    "           out = f(*args, **kwargs).sum()\n",
    "           out.backward()\n",
    "           computed_grads = [a.grad.numpy() for a in args]\n",
    "       error = sum(\n",
    "           np.linalg.norm(computed_grads[i] - numerical_grads[i]) for i in range(len(args))\n",
    "       )\n",
    "       assert error < tol\n",
    "       return computed_grads"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}